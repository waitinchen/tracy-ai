"""
ğŸ¤ èªéŸ³å¼•æ“æ•´åˆç¯„ä¾‹

å±•ç¤ºå¦‚ä½•æ•´åˆè‡ªä¸»èªæ°£åˆ¤æ–·ã€æ¨™ç±¤æ˜ å°„å’Œ TTS ç”Ÿæˆ
æ”¯æ´å¤šèªæ¨¡å¼è‡ªå‹•åˆ‡æ›
"""

from modules.autonomous_emotion import autonomous_emotion_route, get_global_agent
from modules.speech_tag_mapper import extract_tags_from_text, map_tags_to_voice_settings
from eleven_tts import generate_speech
from typing import Optional
import re


# èªè¨€æª¢æ¸¬é—œéµå­—
LANGUAGE_KEYWORDS = {
    "zh": {
        "keywords": ["ä½ å¥½", "è¬è¬", "å†è¦‹", "æ˜¯çš„", "ä¸æ˜¯", "ä»€éº¼", "æ€éº¼", "ç‚ºä»€éº¼"],
        "model_id": "eleven_multilingual_v3"  # æ”¯æ´ä¸­æ–‡
    },
    "en": {
        "keywords": ["hello", "thank", "goodbye", "yes", "no", "what", "how", "why"],
        "model_id": "eleven_turbo_v2_5"  # è‹±æ–‡å„ªåŒ–
    },
    "ja": {
        "keywords": ["ã“ã‚“ã«ã¡ã¯", "ã‚ã‚ŠãŒã¨ã†", "ã•ã‚ˆã†ãªã‚‰", "ã¯ã„", "ã„ã„ãˆ", "ä½•", "ã©ã†", "ãªãœ"],
        "model_id": "eleven_multilingual_v3"  # æ”¯æ´æ—¥æ–‡
    },
    "ko": {
        "keywords": ["ì•ˆë…•", "ê°ì‚¬", "ì•ˆë…•íˆ", "ì˜ˆ", "ì•„ë‹ˆ", "ë¬´ì—‡", "ì–´ë–»ê²Œ", "ì™œ"],
        "model_id": "eleven_multilingual_v3"  # æ”¯æ´éŸ“æ–‡
    }
}


def detect_language(text: str) -> str:
    """
    è‡ªå‹•æª¢æ¸¬æ–‡å­—èªè¨€
    
    Args:
        text: è¼¸å…¥æ–‡å­—
        
    Returns:
        èªè¨€ä»£ç¢¼ï¼šzh, en, ja, koï¼Œé è¨­è¿”å› "zh"
    """
    text_lower = text.lower()
    
    # è¨ˆç®—å„èªè¨€çš„åŒ¹é…åˆ†æ•¸
    scores = {}
    for lang_code, lang_info in LANGUAGE_KEYWORDS.items():
        score = sum(1 for keyword in lang_info["keywords"] if keyword.lower() in text_lower)
        scores[lang_code] = score
    
    # æª¢æŸ¥ä¸­æ–‡å­—ç¬¦ï¼ˆCJK çµ±ä¸€æ¼¢å­—ç¯„åœï¼‰
    cjk_pattern = re.compile(r'[\u4e00-\u9fff]')
    if cjk_pattern.search(text):
        scores["zh"] = scores.get("zh", 0) + 10  # å¤§å¹…æé«˜ä¸­æ–‡åˆ†æ•¸
    
    # æª¢æŸ¥æ—¥æ–‡å‡å
    hiragana_pattern = re.compile(r'[\u3040-\u309f]')
    katakana_pattern = re.compile(r'[\u30a0-\u30ff]')
    if hiragana_pattern.search(text) or katakana_pattern.search(text):
        scores["ja"] = scores.get("ja", 0) + 10
    
    # æª¢æŸ¥éŸ“æ–‡
    korean_pattern = re.compile(r'[\uac00-\ud7a3]')
    if korean_pattern.search(text):
        scores["ko"] = scores.get("ko", 0) + 10
    
    # è¿”å›åˆ†æ•¸æœ€é«˜çš„èªè¨€
    if scores:
        detected_lang = max(scores.items(), key=lambda x: x[1])[0]
        if scores[detected_lang] > 0:
            return detected_lang
    
    # é è¨­è¿”å›ä¸­æ–‡
    return "zh"


def get_model_id_for_language(lang: str) -> str:
    """
    æ ¹æ“šèªè¨€ç²å–å°æ‡‰çš„æ¨¡å‹ ID
    
    Args:
        lang: èªè¨€ä»£ç¢¼
        
    Returns:
        æ¨¡å‹ ID
    """
    return LANGUAGE_KEYWORDS.get(lang, LANGUAGE_KEYWORDS["zh"])["model_id"]


def speak_with_autonomous_emotion(
    text: str,
    autonomy_level: float = 0.7,
    use_llm: bool = True,
    output_filename: str = "output.mp3",
    auto_detect_language: bool = True,
    language: Optional[str] = None,
    intensity: Optional[float] = None
) -> bool:
    """
    å®Œæ•´çš„èªéŸ³ç”Ÿæˆæµç¨‹ï¼š
    1. è‡ªä¸»èªæ°£åˆ¤æ–·
    2. æ¨™ç±¤æ˜ å°„åˆ°è²éŸ³åƒæ•¸
    3. TTS ç”Ÿæˆ
    
    Args:
        text: è¼¸å…¥æ–‡å­—
        autonomy_level: è‡ªä¸»ç¨‹åº¦
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM
        output_filename: è¼¸å‡ºæª”æ¡ˆåç¨±
        
    Returns:
        æˆåŠŸè¿”å› True
    """
    print(f"ğŸ“ è¼¸å…¥æ–‡å­—ï¼š{text}")
    
    # 1. è‡ªä¸»èªæ°£åˆ¤æ–·
    agent = get_global_agent(autonomy_level=autonomy_level)
    tagged_text = autonomous_emotion_route(
        text,
        autonomy_level=autonomy_level,
        use_llm=use_llm,
        agent=agent
    )
    
    print(f"ğŸ·ï¸  æ¨™ç±¤å¾Œæ–‡å­—ï¼š{tagged_text}")
    
    # 2. æå–æ¨™ç±¤ä¸¦æ˜ å°„åˆ°è²éŸ³åƒæ•¸
    tags = extract_tags_from_text(tagged_text)
    voice_settings = map_tags_to_voice_settings(tags)
    
    print(f"ğŸµ è²éŸ³è¨­å®šï¼š{voice_settings}")
    
    # 3. ç”ŸæˆèªéŸ³ï¼ˆä½¿ç”¨æ˜ å°„çš„è²éŸ³åƒæ•¸ï¼‰
    success = generate_speech(
        text=tagged_text,  # ä¿æŒæ¨™ç±¤åœ¨æ–‡å­—ä¸­ï¼ˆElevenLabs æœƒè™•ç†ï¼‰
        filename=output_filename,
        use_tag_mapper=False,  # å·²ç¶“æ‰‹å‹•æ˜ å°„ï¼Œä¸éœ€è¦å†æ¬¡æ˜ å°„
        voice_settings=voice_settings  # ä½¿ç”¨æ˜ å°„çš„åƒæ•¸
    )
    
    if success:
        print(f"âœ… èªéŸ³å·²ç”Ÿæˆï¼š{output_filename}")
        return True
    else:
        print("âŒ èªéŸ³ç”Ÿæˆå¤±æ•—")
        return False


def analyze_emotion(text: str) -> dict:
    """
    åˆ†ææ–‡å­—çš„æƒ…ç·’ï¼ˆèˆ‡ç”¨æˆ¶æä¾›çš„ç¯„ä¾‹æ ¼å¼ä¸€è‡´ï¼‰
    
    Args:
        text: è¼¸å…¥æ–‡å­—
        
    Returns:
        {"text": "...", "tags": [...]}
    """
    agent = get_global_agent()
    tagged_text = agent.process_text(text, use_llm=True)
    tags = extract_tags_from_text(tagged_text)
    
    # ç§»é™¤æ¨™ç±¤ï¼Œåªä¿ç•™ç´”æ–‡å­—
    clean_text = re.sub(r'\[([^\]]+)\]\s*', '', tagged_text).strip()
    
    return {
        "text": clean_text,
        "tags": tags
    }


# æ¸¬è©¦å‡½æ•¸
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¤ èªéŸ³å¼•æ“æ•´åˆæ¸¬è©¦ï¼ˆå¤šèªæ¨¡å¼ï¼‰")
    print("=" * 60)
    print()
    
    # æ¸¬è©¦æ¡ˆä¾‹ï¼ˆå¤šèªè¨€ï¼‰
    test_cases = [
        ("ä½ çŸ¥é“å—ï¼Ÿæˆ‘çœŸçš„å¥½æ„Ÿå‹•ã€‚", "zh"),
        ("Hello, how are you?", "en"),
        ("ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ", "ja"),
        ("ì•ˆë…•í•˜ì„¸ìš”, ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?", "ko"),
    ]
    
    for text, expected_lang in test_cases:
        print(f"\næ¸¬è©¦ï¼š{text}")
        print("-" * 60)
        
        # èªè¨€æª¢æ¸¬
        detected_lang = detect_language(text)
        model_id = get_model_id_for_language(detected_lang)
        print(f"æª¢æ¸¬èªè¨€ï¼š{detected_lang}ï¼ˆé æœŸï¼š{expected_lang}ï¼‰")
        print(f"ä½¿ç”¨æ¨¡å‹ï¼š{model_id}")
        
        # åˆ†ææƒ…ç·’
        emotion_result = analyze_emotion(text)
        print(f"åˆ†æçµæœï¼š{emotion_result}")
        
        # ç”ŸæˆèªéŸ³ï¼ˆå¯é¸ï¼Œéœ€è¦ API Keyï¼‰
        # speak_with_autonomous_emotion(
        #     text,
        #     output_filename=f"test_{detected_lang}.mp3",
        #     auto_detect_language=True
        # )


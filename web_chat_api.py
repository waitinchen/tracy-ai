"""
ğŸŒ Web å°è©±ç•Œé¢å¾Œç«¯ API

æä¾›èªéŸ³å°è©±çš„ Web API ç«¯é»
"""

import os
import uuid
import time
from pathlib import Path
from typing import Optional, Dict
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from dotenv import load_dotenv

load_dotenv()

# å°å…¥æ¨¡çµ„
import sys
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from modules.llm_emotion_router import llm_emotion_route
from modules.autonomous_emotion import autonomous_emotion_route, get_global_agent
from modules.speech_tag_mapper import extract_tags_from_text, map_tags_to_voice_settings
from modules.soft_ling import (
    process_with_soft_ling,
    detect_soft_ling_invocation,
    get_soft_ling_opening,
    generate_soft_ling_reply,
)
from eleven_tts import generate_speech, API_KEY, VOICE_ID
import requests

app = FastAPI(
    title="é»ƒè“‰èªéŸ³å°è©±ç³»çµ± Web API",
    description="æä¾›èªéŸ³å°è©±çš„ Web ç•Œé¢ API",
    version="1.0.0"
)

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¨­å®šç›®éŒ„
AUDIO_DIR = Path("public/audio")
AUDIO_DIR.mkdir(parents=True, exist_ok=True)
STATIC_DIR = Path("web_static")
STATIC_DIR.mkdir(parents=True, exist_ok=True)

# æ›è¼‰éœæ…‹æª”æ¡ˆ
app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")


class ChatRequest(BaseModel):
    """å°è©±è«‹æ±‚æ¨¡å‹"""
    text: str
    use_llm: bool = True
    provider: str = "openai"
    autonomy_mode: bool = True  # æ˜¯å¦ä½¿ç”¨è‡ªä¸»æ¨¡å¼
    autonomy_level: float = 0.7  # è‡ªä¸»ç¨‹åº¦ï¼ˆ0.0-1.0ï¼‰
    use_soft_ling: bool = True  # æ˜¯å¦ä½¿ç”¨èŠ±å°è»Ÿæ¨¡å¼


class ChatResponse(BaseModel):
    """å°è©±å›æ‡‰æ¨¡å‹"""
    status: str
    text: str
    tagged_text: str
    audio_url: str
    message: str
    autonomy_stats: Optional[Dict] = None  # è‡ªä¸»æ±ºç­–çµ±è¨ˆï¼ˆå¯é¸ï¼‰
    is_invocation: bool = False  # æ˜¯å¦ç‚ºå¬å–šå’’èª
    agent_name: str = "é»ƒè“‰"  # ä»£ç†åç¨±
    opening: Optional[str] = None  # é–‹éˆèªï¼ˆå¯é¸ï¼‰


@app.get("/")
async def root():
    """é‡å®šå‘åˆ°å°è©±é é¢"""
    return FileResponse(STATIC_DIR / "index.html")


@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    è™•ç†å°è©±è«‹æ±‚
    
    æµç¨‹ï¼šæ–‡å­— â†’ èªæ°£åˆ¤æ–· â†’ èªéŸ³ç”¢ç”Ÿ
    """
    try:
        # æª¢æŸ¥æ˜¯å¦ç‚ºèŠ±å°è»Ÿå¬å–šå’’èª
        is_invocation = detect_soft_ling_invocation(request.text) if request.use_soft_ling else False

        # ç”¢ç”Ÿå›æ‡‰æ–‡å­—ï¼ˆèŠ±å°è»Ÿæ¨¡å¼æ™‚æ‰å•Ÿç”¨ LLMï¼‰
        reply_text = request.text
        if request.use_soft_ling and request.use_llm:
            reply_text = generate_soft_ling_reply(request.text, provider=request.provider)

        # 1. èªæ°£åˆ¤æ–·ï¼ˆèŠ±å°è»Ÿæ¨¡å¼å„ªå…ˆï¼‰
        if request.use_soft_ling:
            # ä½¿ç”¨èŠ±å°è»Ÿæ¨¡å¼
            soft_ling_result = process_with_soft_ling(
                reply_text,
                use_llm=request.use_llm
            )
            tagged_text = soft_ling_result["tagged_text"]
            voice_settings = soft_ling_result["voice_settings"]
        elif request.autonomy_mode:
            # ä½¿ç”¨è‡ªä¸»æ¨¡å¼ï¼šå°è»Ÿè‡ªå·±æ±ºå®šæ˜¯å¦ä½¿ç”¨èªæ°£
            agent = get_global_agent(autonomy_level=request.autonomy_level)
            tagged_text = autonomous_emotion_route(
                reply_text,
                autonomy_level=request.autonomy_level,
                use_llm=request.use_llm,
                agent=agent
            )
            # æå–æ¨™ç±¤ä¸¦æ˜ å°„åˆ°è²éŸ³åƒæ•¸
            tags = extract_tags_from_text(tagged_text)
            voice_settings = map_tags_to_voice_settings(tags)
        elif request.use_llm:
            # å‚³çµ±æ¨¡å¼ï¼šä½¿ç”¨ LLM åˆ¤æ–·
            tagged_text = llm_emotion_route(
                reply_text,
                provider=request.provider,
                fallback_to_rule=True
            )
            tags = extract_tags_from_text(tagged_text)
            voice_settings = map_tags_to_voice_settings(tags)
        else:
            # è¦å‰‡å¼åˆ¤æ–·
            from emotion_tag_engine import insert_emotion_tags
            tagged_text = insert_emotion_tags(reply_text)
            tags = extract_tags_from_text(tagged_text)
            voice_settings = map_tags_to_voice_settings(tags)
        
        # 2. ç”¢ç”ŸèªéŸ³
        filename = f"chat_{uuid.uuid4().hex[:8]}.mp3"
        filepath = AUDIO_DIR / filename
        
        voice_id = VOICE_ID
        if not voice_id:
            raise HTTPException(status_code=500, detail="æœªè¨­å®š Voice ID")
        
        url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream"
        headers = {
            "xi-api-key": API_KEY,
            "Content-Type": "application/json"
        }
        payload = {
            "model_id": "eleven_turbo_v2_5",
            "text": tagged_text,  # ä¿æŒæ¨™ç±¤åœ¨æ–‡å­—ä¸­
            "voice_settings": voice_settings  # ä½¿ç”¨æ˜ å°„çš„è²éŸ³åƒæ•¸
        }
        
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"ElevenLabs API éŒ¯èª¤ï¼š{response.text}"
            )
        
        # 3. å„²å­˜éŸ³è¨Šæª”æ¡ˆ
        with open(filepath, "wb") as f:
            f.write(response.content)
        
        # 4. å›å‚³çµæœ
        audio_url = f"/audio/{filename}"
        
        # ç²å–è‡ªä¸»æ±ºç­–çµ±è¨ˆï¼ˆå¦‚æœä½¿ç”¨è‡ªä¸»æ¨¡å¼ä¸”éèŠ±å°è»Ÿæ¨¡å¼ï¼‰
        autonomy_stats = None
        if request.autonomy_mode and not request.use_soft_ling:
            agent = get_global_agent(autonomy_level=request.autonomy_level)
            autonomy_stats = agent.get_autonomy_stats()
        
        # å¦‚æœæ˜¯å¬å–šå’’èªï¼Œæ·»åŠ é–‹éˆèªæ¨™è¨˜
        response_data = {
            "status": "success",
            "text": reply_text,
            "tagged_text": tagged_text,
            "audio_url": audio_url,
            "message": "èªéŸ³ç”¢ç”ŸæˆåŠŸ",
            "autonomy_stats": autonomy_stats,
            "is_invocation": is_invocation,
            "agent_name": "èŠ±å°è»Ÿ" if request.use_soft_ling else "é»ƒè“‰"
        }
        
        if is_invocation and request.use_soft_ling:
            response_data["opening"] = get_soft_ling_opening()
        
        return ChatResponse(**response_data)
        
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"âŒ API éŒ¯èª¤è©³æƒ…:\n{error_detail}")
        raise HTTPException(status_code=500, detail=f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")


@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    """ç›´æ¥è¿”å›éŸ³è¨Šæµ"""
    try:
        # èªæ°£åˆ¤æ–·ï¼ˆè‡ªä¸»æ¨¡å¼æˆ–å‚³çµ±æ¨¡å¼ï¼‰
        if request.autonomy_mode:
            agent = get_global_agent(autonomy_level=request.autonomy_level)
            tagged_text = autonomous_emotion_route(
                request.text,
                autonomy_level=request.autonomy_level,
                use_llm=request.use_llm,
                agent=agent
            )
        elif request.use_llm:
            tagged_text = llm_emotion_route(
                request.text,
                provider=request.provider,
                fallback_to_rule=True
            )
        else:
            from emotion_tag_engine import insert_emotion_tags
            tagged_text = insert_emotion_tags(request.text)
        
        # å‘¼å« ElevenLabs API
        voice_id = VOICE_ID
        if not voice_id:
            raise HTTPException(status_code=500, detail="æœªè¨­å®š Voice ID")
        
        url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream"
        headers = {
            "xi-api-key": API_KEY,
            "Content-Type": "application/json"
        }
        payload = {
            "model_id": "eleven_turbo_v2_5",
            "text": tagged_text,
            "voice_settings": {
                "stability": 0.4,
                "similarity_boost": 0.8,
                "style": 0.9,
                "use_speaker_boost": True
            }
        }
        
        response = requests.post(url, headers=headers, json=payload, stream=True, timeout=30)
        
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"ElevenLabs API éŒ¯èª¤ï¼š{response.text}"
            )
        
        return StreamingResponse(
            response.iter_content(chunk_size=8192),
            media_type="audio/mpeg",
            headers={
                "Content-Disposition": f'attachment; filename="huangrong_chat.mp3"'
            }
        )
        
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"âŒ API éŒ¯èª¤è©³æƒ…:\n{error_detail}")
        raise HTTPException(status_code=500, detail=f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")


@app.get("/audio/{filename}")
async def serve_audio(filename: str):
    """æä¾›éŸ³è¨Šæª”æ¡ˆä¸‹è¼‰"""
    filepath = AUDIO_DIR / filename
    if not filepath.exists():
        raise HTTPException(status_code=404, detail="æª”æ¡ˆä¸å­˜åœ¨")
    
    return FileResponse(filepath, media_type="audio/mpeg")


@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    return {
        "status": "healthy",
        "api_key_set": bool(API_KEY),
        "voice_id_set": bool(VOICE_ID)
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)

